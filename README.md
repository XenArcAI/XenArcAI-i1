##Xenith: A High-Performance Transformer Model


Unleash the Power of Advanced AI with Xenith v2.0.0 ("Beast")

#ðŸŒŸ Overview
Xenith is a cutting-edge transformer model engineered by the team at xAI for unparalleled performance in text classification and beyond. Combining a robust Mixture of Experts (MoE) architecture, efficient attention mechanisms, and specialized support for large contexts and multilingual capabilities (like Hinglish), Xenith is optimized for TPUs and designed to push the boundaries of modern AI.

##Key Features
-Large Context Handling: Supports sequences up to 32,768 tokens with advanced chunking and summarization.
-Mixture of Experts (MoE): Dynamically routes tokens to 64 experts, activating 8 per token for efficiency and power.
-Efficient Attention: Sparse and full attention options, with dynamic window sizing for optimal performance.
-Hinglish Support: Seamlessly processes mixed Hindi-English text with a combined tokenizer.
-TPU Optimization: Ready for PyTorch XLA, ensuring lightning-fast training and inference.
-LoRA Adaptation: Low-Rank Adaptation for efficient fine-tuning across attention and feed-forward layers.

##ðŸš€ Getting Started

#Prerequisites
Python 3.8+
PyTorch (with optional XLA support for TPUs)
Hugging Face Transformers (pip install transformers)
Optional: IndicNLP for Hindi support (pip install git+https://github.com/anoopkunchukuttan/indic_nlp_library.git)
Optional: Safetensors for optimized model saving (pip install safetensors)
